{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [\"normalizaiton_task\",\"nli_task\", \"ner\", \"QA_pair\",\"QA_multi-choice\", \n",
    "                    \"relation_extraction\", \"sentence_similarity\", \"summary_task\",\"\", \"text_classification\", \"MRC\", \"entity_link\",\"text2sql\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_pth = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/dataset_tsv/{}\"\n",
    "w = open(base_pth.format(\"all_train.tsv\"), \"a\")\n",
    "all_len = []\n",
    "tasks_len = {}\n",
    "for i in all:\n",
    "    datasets_pth = base_pth.format(i)\n",
    "    num = 0 \n",
    "    for dataset_name in os.listdir(datasets_pth):\n",
    "\n",
    "        source_pth = datasets_pth +\"/\"+dataset_name+\"/\"+\"train.tsv\"\n",
    "        with open(source_pth, \"r\") as f:\n",
    "            length = len([i for i in f])\n",
    "            if length==0:\n",
    "                print(source_pth)\n",
    "            all_len.append(length)\n",
    "            num+=length\n",
    "    tasks_len[i]=num\n",
    "all_len            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "base_pth = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/dataset_tsv/{}\"\n",
    "w = open(base_pth.format(\"all_train.tsv\"), \"a\")\n",
    "test_write = open(base_pth.format(\"all_test.tsv\"), \"a\")\n",
    "\n",
    "for i in all:\n",
    "    if tasks_len[i]<20000:\n",
    "        task_weight = 200000.0/20000.0\n",
    "    elif tasks_len[i]>1000000:\n",
    "        task_weight = 200000.0/1000000.0\n",
    "    else:\n",
    "        task_weight = 200000.0/float(tasks_len[i])\n",
    "    datasets_pth = base_pth.format(i)\n",
    "    for dataset_name in os.listdir(datasets_pth):\n",
    "        source_pth = datasets_pth +\"/\"+dataset_name+\"/\"+\"train.tsv\"\n",
    "        with open(source_pth, \"r\") as f:\n",
    "            test_num = 0\n",
    "            for line in f:\n",
    "                test_num+=1\n",
    "                if len(line.strip().split(\"\\t\")[0].split(\" \"))>=512:\n",
    "                    continue\n",
    "                if len(line.strip().split(\"\\t\"))!=2:\n",
    "                    continue\n",
    "                if test_num<100:\n",
    "                    test_write.write(line.replace(\"\\n\", \"\"))\n",
    "                    test_write.write(\"\\n\")\n",
    "                    continue\n",
    "                weight.append(task_weight)\n",
    "                w.write(line.replace(\"\\n\", \"\"))\n",
    "                w.write(\"\\n\")       \n",
    "w.close()\n",
    "test_write.close()\n",
    "with open(\"weight\", \"w\") as f:\n",
    "    json.dump(weight,f, indent=2, ensure_ascii=False)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_pth = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/dataset_tsv/all_train.tsv\"\n",
    "with open(traindata_pth,\"r\") as f:\n",
    "    num = 0\n",
    "    for i in f:\n",
    "        num+=1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pth = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/blurb_test_dataset\"\n",
    "import os\n",
    "import json\n",
    "test_w = open(\"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/dataset_tsv/all_test.tsv\", \"a\")\n",
    "for _,dirs,_ in os.walk(test_pth):\n",
    "    for dir_pth in dirs:\n",
    "        test_dataset_pth = test_pth+\"/\"+dir_pth\n",
    "        file_name = os.listdir(test_dataset_pth)[0]\n",
    "        with open(test_dataset_pth+\"/\"+file_name, \"r\") as f:\n",
    "                content = json.load(f)\n",
    "                for line in content[:100]:\n",
    "                    test_w.write(line.replace(\"\\n\", \"\"))\n",
    "                    test_w.write(\"\\n\")\n",
    "test_w.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/dataset_tsv/all_train.tsv\"\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\",names=[\"source_text\", \"target_text\"], low_memory=False, on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.dropna()\n",
    "print(len(train_df2))\n",
    "print(len(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.values.tolist():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes import PyTorchDataModule\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\n",
    "test_path =  \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/dataset_tsv/all_test.tsv\"\n",
    "\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\",names=[\"source_text\", \"target_text\"], low_memory=False)\n",
    "\n",
    "gdata = PyTorchDataModule(test_df, tokenizer, 512,512)\n",
    "# data_loader = DataLoader(gdata,batch_size =8,sampler= WeightedRandomSampler([0.1,0.1,0.5,0.5],num_samples=20, replacement=True))\n",
    "# for i in data_loader:\n",
    "#     print(i)\n",
    "#     break\n",
    "len(gdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "content = [i for i in os.listdir(\"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/zh_traindataset\")]\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue_intention_classifier\n"
     ]
    }
   ],
   "source": [
    "path_ex = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/en.xlsx\"\n",
    "import pandas as pd\n",
    "d = pd.read_excel(path_ex, sheet_name=\"zh\", engine=\"openpyxl\")\n",
    "zh_datasetnames = d[\"数据集\"].values.tolist()\n",
    "for name in zh_datasetnames:\n",
    "    if name not in content: \n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toadexport1\n"
     ]
    }
   ],
   "source": [
    "for it in content:\n",
    "    if it not in zh_datasetnames:\n",
    "        print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.t5.modeling_t5.T5ForConditionalGeneration"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5PreTrainedModel, T5ForConditionalGeneration\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\", return_dict=True)\n",
    "T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"T5 Model with a `language modeling` head on top.\\n\\n    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\\n    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\\n    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\\n    text-to-text denoising generative setting.\\n\\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\\n    etc.)\\n\\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\\n    and behavior.\\n\\n    Parameters:\\n        config ([`T5Config`]): Model configuration class with all the parameters of the model.\\n            Initializing with a config file does not load the weights associated with the model, only the\\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.__doc__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('t0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "462d7afb82e73546fb32fc68c91be82fe4847e6fdf969ffd1e2ed36758d69187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
