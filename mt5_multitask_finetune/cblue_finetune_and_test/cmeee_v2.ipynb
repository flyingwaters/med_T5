{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgxQv5QTPqMS"
      },
      "source": [
        "## Set Up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hglvOHLCd1sg"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "import gin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7wZkDsohpUE"
      },
      "source": [
        "## Set UP TPU Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE1ujiZOhlyo"
      },
      "outputs": [],
      "source": [
        "tf.disable_v2_behavior()\n",
        "gin.parse_config_file(\n",
        "        '/raid/yiptmp/huggingface-models/cblue_finetune/mt5_3b_chip_mdcfnpc/operative_config.gin'\n",
        ")\n",
        "# \n",
        "vocab = \"/raid/yiptmp/huggingface-models/mt5_vocab/sentencepiece.model\"\n",
        "\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjtB4YHVkeMC"
      },
      "source": [
        "### chip mdcfnpc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpwpnxiFgcnC"
      },
      "outputs": [],
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            '/raid/yiptmp/nlp_prepare_dataset/med0_dataset/train_cblue_v2/chip_mdcfnpc/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            '/raid/yiptmp/nlp_prepare_dataset/med0_dataset/test_cblue_v2/chip_mdcfnpc/validation.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXaiyZjpgeM5"
      },
      "outputs": [],
      "source": [
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\": normalize_text(ex[\"input\"]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dbe7-xGjxHG"
      },
      "outputs": [],
      "source": [
        "t5.data.TaskRegistry.remove('chip_mdcfnpc')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"chip_mdcfnpc\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odi8I-pYkn4j"
      },
      "outputs": [],
      "source": [
        "t5.data.MixtureRegistry.remove(\"ner_all\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"ner_all\",\n",
        "    [\n",
        "     \"chip_mdcfnpc\"\n",
        "     ],\n",
        "     default_rate=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11UkumTlDMh"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1vtASB-lCBQ"
      },
      "outputs": [],
      "source": [
        "MODEL_SIZE = \"base\"\n",
        "BASE_PRETRAINED_DIR = \"/raid/yiptmp/huggingface-models/cblue_finetune/mt5_3b_chip_mdcfnpc\"\n",
        "MODEL_DIR = BASE_PRETRAINED_DIR\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=None,\n",
        "    mesh_shape=\"model:2, batch:4\",\n",
        "    mesh_devices=['gpu:0', 'gpu:1', 'gpu:2', 'gpu:3', 'gpu:4','gpu:5','gpu:6', 'gpu:7'],\n",
        "    batch_size=8,\n",
        "    sequence_length = {'inputs': 1024, 'targets': 1024},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=60000,\n",
        "    keep_checkpoint_max=1,\n",
        "    iterations_per_loop=10,#\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBu7B_Vdl5BB"
      },
      "source": [
        "## Finetune "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MjNd4BIKl06z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:training_loop marked as finished\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3023/3856957959.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmixture_or_task_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ner_all\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpretrained_model_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfinetune_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFINETUNE_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/t5/models/mtf_model.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(self, mixture_or_task_name, finetune_steps, pretrained_model_dir, pretrained_checkpoint_step, split)\u001b[0m\n\u001b[1;32m    385\u001b[0m     self.train(mixture_or_task_name, checkpoint_step + finetune_steps,\n\u001b[1;32m    386\u001b[0m                \u001b[0minit_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                split=split)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m   def predict(self, input_file, output_file, checkpoint_steps=-1,\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/t5/models/mtf_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mixture_or_task_name, steps, init_checkpoint, split)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensemble_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         dataset_split=split)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   def _predict_or_score_fn(self,\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(estimator, vocabulary, sequence_length, batch_size, train_dataset_fn, train_steps, ensemble_inputs, dataset_split, skip_seen_data, seen_data_init_step, checkpoint_input_pipeline)\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointInputPipelineHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2176\u001b[0;31m   \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   3082\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3084\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[0;32m-> 1204\u001b[0;31m                                            self.config)\n\u001b[0m\u001b[1;32m   1205\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   2919\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m       return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\n\u001b[0;32m-> 2921\u001b[0;31m                                                       config)\n\u001b[0m\u001b[1;32m   2922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2923\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn_for_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config, params)\u001b[0m\n\u001b[1;32m   3177\u001b[0m           \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running %s on CPU/GPU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3178\u001b[0m           estimator_spec = model_fn_wrapper.call_without_tpu(\n\u001b[0;32m-> 3179\u001b[0;31m               features, labels, is_export_mode=is_export_mode)\n\u001b[0m\u001b[1;32m   3180\u001b[0m           if (self._log_every_n_steps is not None or\n\u001b[1;32m   3181\u001b[0m               self._log_every_n_secs is not None):\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mcall_without_tpu\u001b[0;34m(self, features, labels, is_export_mode)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_without_tpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_export_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_export_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_export_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_embedding_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_dummy_table_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, is_export_mode)\u001b[0m\n\u001b[1;32m   2029\u001b[0m       \u001b[0m_add_item_to_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_CTX_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0mestimator_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m     if (running_on_cpu and\n\u001b[1;32m   2033\u001b[0m         isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)):  # pylint: disable=protected-access\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py\u001b[0m in \u001b[0;36mmy_model_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    932\u001b[0m           \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmesh_impl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m           \u001b[0mautostack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautostack\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m           log_file=model_info_file)\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0mtf_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlowering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_to_tf_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, mesh_to_impl, autostack, log_file)\u001b[0m\n\u001b[1;32m    727\u001b[0m       \u001b[0;31m# tf.logging.info(\"Lowering operation %s\" % op.to_string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         self.add_counter(\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/ops.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(self, lowering)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         [x.shape for x in self.inputs], intersection_shape, mesh_impl)\n\u001b[1;32m   2871\u001b[0m     y = mesh_impl.slicewise(\n\u001b[0;32m-> 2872\u001b[0;31m         einsum_slice_fn, *[lowering.tensors[x] for x in self.inputs])\n\u001b[0m\u001b[1;32m   2873\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduced_mesh_axes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0madd_counter_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/placement_mesh_impl.py\u001b[0m in \u001b[0;36mslicewise\u001b[0;34m(self, fn, *inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     inputs = [x.tensor_list if isinstance(x, self.LaidOutTensor)\n\u001b[1;32m    172\u001b[0m               else [x] * len(self.devices) for x in inputs]\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_list_of_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/ops.py\u001b[0m in \u001b[0;36mparallel\u001b[0;34m(devices, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5715\u001b[0m         \u001b[0mmy_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5716\u001b[0m         \u001b[0mmy_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5717\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmy_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5718\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/mesh_tensorflow/ops.py\u001b[0m in \u001b[0;36meinsum_slice_fn_naive\u001b[0;34m(*slices)\u001b[0m\n\u001b[1;32m   2814\u001b[0m     return reduce_slice_fn(functools.reduce(tf.multiply, [\n\u001b[1;32m   2815\u001b[0m         \u001b[0m_expand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2816\u001b[0;31m         for x, input_shape in zip(slices, input_shapes)]))\n\u001b[0m\u001b[1;32m   2817\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfull_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m     \u001b[0;31m# it is not wasteful of space to broadcast fully and then reduce.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    506\u001b[0m   \"\"\"\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6175\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 6176\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6177\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6178\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 1975\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1781\u001b[0m   \u001b[0;31m# TODO(skyewm): op_def_library.apply_op() flattens the incoming inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m   \u001b[0;31m# Refactor so we don't have to do this here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct_sequence_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m   op_desc = pywrap_tf_session.TF_NewOperation(graph._c_graph,\n",
            "\u001b[0;32m~/anaconda3/envs/t0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_reconstruct_sequence_inputs\u001b[0;34m(op_def, inputs, attrs)\u001b[0m\n\u001b[1;32m   6893\u001b[0m   \u001b[0mgrouped_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6894\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6895\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0minput_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6896\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_attr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6897\u001b[0m       \u001b[0minput_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "FINETUNE_STEPS = 60000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"ner_all\",\n",
        "    pretrained_model_dir=MODEL_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynIhfRMDm3pO"
      },
      "source": [
        "## Export Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43yafDz9l7QN"
      },
      "outputs": [],
      "source": [
        "export_dir = os.path.join(MODEL_DIR, \"export\")\n",
        "\n",
        "model.batch_size = 1 # make one prediction per call\n",
        "saved_model_path = model.export(\n",
        "    export_dir,\n",
        "    checkpoint_step=-1,  # use most recent\n",
        "    beam_size=1,  # no beam search\n",
        "    temperature=0.0,  # sample according to predicted distribution\n",
        ")\n",
        "print(\"Model saved to:\", saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert 1==2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 重新计算 指标 根据各个模板"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh0hQPtPNgkJ"
      },
      "source": [
        "## Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1FjOX1K6x2J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text  # Required to run exported model.\n",
        "print(tf.__version__)\n",
        "def load_predict_fn(model_path):\n",
        "  if tf.executing_eagerly():\n",
        "    print(\"Loading SavedModel in eager mode.\")\n",
        "    imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "    return lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n",
        "  else:\n",
        "    print(\"Loading SavedModel in tf 1.x graph mode.\")\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    sess = tf.compat.v1.Session()\n",
        "    meta_graph_def = tf.compat.v1.saved_model.load(sess, [\"serve\"], model_path)\n",
        "    signature_def = meta_graph_def.signature_def[\"serving_default\"]\n",
        "    return lambda x: sess.run(\n",
        "        fetches=signature_def.outputs[\"outputs\"].name, \n",
        "        feed_dict={signature_def.inputs[\"input\"].name: x}\n",
        "    )\n",
        "\n",
        "predict_fn = load_predict_fn(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCGXX-yvyHTF"
      },
      "outputs": [],
      "source": [
        "# imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "# test = lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeN89QnANzMe"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RYeWZPLUb6c"
      },
      "outputs": [],
      "source": [
        "# tasks = [\n",
        "#          ['NCBI-disease', \"ncbi_ner\"], \n",
        "#          ['BC5CDR-disease', \"bc5cdr_disease_ner\"], \n",
        "#          ['BC5CDR-chem', \"bc5cdr_chem_ner\"], \n",
        "#          ['BC4CHEMD', 'bc4chemd_ner'], \n",
        "#          ['BC2GM', 'bc2gm_ner'], \n",
        "#          ['JNLPBA', 'jnlpba_ner'], \n",
        "#          ['linnaeus', 'linnaeus_ner'], \n",
        "#          ['s800', 's800_ner']\n",
        "#          ]\n",
        "# output_dir = 'ner_all_pubmed_v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZxUTGJmPdYO"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "# question_1 = \"Emerin is a nuclear membrane protein which is missing or defective in Emery-Dreifuss muscular dystrophy (EDMD). It is one member of a family of lamina-associated proteins which includes LAP1, LAP2 and lamin B receptor (LBR). A panel of 16 monoclonal antibodies (mAbs) has been mapped to six specific sites throughout the emerin molecule using phage-displayed peptide libraries and has been used to localize emerin in human and rabbit heart. Several mAbs against different emerin epitopes did not recognize intercalated discs in the heart, though they recognized cardiomyocyte nuclei strongly, both at the rim and in intranuclear spots or channels. A polyclonal rabbit antiserum against emerin did recognize both nuclear membrane and intercalated discs but, after affinity purification against a pure-emerin band on a western blot, it stained only the nuclear membrane. These results would not be expected if immunostaining at intercalated discs were due to a product of the emerin gene and, therefore, cast some doubt upon the hypothesis that cardiac defects in EDMD are caused by absence of emerin from intercalated discs. Although emerin was abundant in the membranes of cardiomyocyte nuclei, it was absent from many non-myocyte cells in the heart. This distribution of emerin was similar to that of lamin A, a candidate gene for an autosomal form of EDMD. In contrast, lamin B1 was absent from cardiomyocyte nuclei, showing that lamin B1 is not essential for localization of emerin to the nuclear lamina. Lamin B1 is also almost completely absent from skeletal muscle nuclei. In EDMD, the additional absence of lamin B1 from heart and skeletal muscle nuclei which already lack emerin may offer an alternative explanation of why these tissues are particularly affected..\" \n",
        "# question_2 = \"Molecular analysis of the APC gene in 205 families: extended genotype-phenotype correlations in FAP and evidence for the role of APC amino acid changes in colorectal cancer predisposition.\" \n",
        "# question_3 = \"Who are the 4 members of The Beatles?\" \n",
        "# question_4 = \"How many teeth do humans have?\"\n",
        "\n",
        "# questions = [question_2]\n",
        "\n",
        "\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  task = t[1]\n",
        "  input_file = task + '_predict_input.txt'\n",
        "  output_file = task + '_predict_output.txt'\n",
        "\n",
        "\n",
        "  # Write out the supplied questions to text files.\n",
        "  predict_inputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, input_file)\n",
        "  predict_outputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, output_dir, MODEL_SIZE, output_file)\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        # Select the most probable output token at each step.\n",
        "        vocabulary=t5.data.SentencePieceVocabulary(vocab),\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + task)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laXUsUBeZ2-m"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv8k4M1AZ1V1"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval\n",
        "import nltk\n",
        "from seqeval.metrics import f1_score, accuracy_score, classification_report, recall_score, precision_score\n",
        "import re\n",
        "import os\n",
        "tasks = [\n",
        "         ['NCBI-disease', \"ncbi_ner\"], \n",
        "         ['BC5CDR-disease', \"bc5cdr_disease_ner\"], \n",
        "         ['BC5CDR-chem', \"bc5cdr_chem_ner\"], \n",
        "         ['BC4CHEMD', 'bc4chemd_ner'], \n",
        "         ['BC2GM', 'bc2gm_ner'], \n",
        "         ['JNLPBA', 'jnlpba_ner'], \n",
        "         ['linnaeus', 'linnaeus_ner'], \n",
        "         ['s800', 's800_ner']\n",
        "         ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvwgJlJn3Tlw"
      },
      "outputs": [],
      "source": [
        "checkpoint = 249600\n",
        "for t in tasks:\n",
        "  !gsutil cp gs://t5_training/t5-data/bio_data/{t[0]}/{output_dir}/base/{t[1]}_predict_output.txt-* .\n",
        "  # !gsutil cp gs://t5_training/t5-data/bio_data/{t[0]}/{t[1]}_predict_output.txt-* .\n",
        "  # t5_training/t5-data/bio_data/BC4CHEMD/predicted_output_original_model/base\n",
        "  !gsutil cp gs://t5_training/t5-data/bio_data/{t[0]}/{t[1]}_actual_output.txt .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWnAELJ-aAm0"
      },
      "outputs": [],
      "source": [
        "def convert_BIO_labels(filename):\n",
        "    result_labels = []\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        cnt = 0\n",
        "        for line in file:\n",
        "            line = re.sub(r'\\*(\\w+)', r'\\1*', line)\n",
        "            tokens = re.sub(r'[!\"#$%&\\'()+,-.:;<=>?@[\\\\\\]^_`{\\|}~⁇]', ' ', line.strip()).split()\n",
        "            # 替换后\n",
        "            # \n",
        "            seq_label = []\n",
        "            start_entity = 0\n",
        "            entity_type = 'O'\n",
        "            for idx, token in enumerate(tokens):\n",
        "                if token.endswith('*'):\n",
        "                    start_entity += 1 if (start_entity == 0 or token[:-1] != entity_type) else -1\n",
        "                    entity_type = token[:-1]\n",
        "                else:\n",
        "                    if start_entity == 0:\n",
        "                        seq_label.append('O')\n",
        "                        entity_type = 'O'\n",
        "                    elif start_entity < 0:\n",
        "                        raise \"Something errors\"\n",
        "                    else:\n",
        "                        if tokens[idx - 1].endswith('*'):\n",
        "                            seq_label.append('B-' + entity_type.upper())\n",
        "                        else:\n",
        "                            seq_label.append('I-' + entity_type.upper())\n",
        "\n",
        "            result_labels.append(seq_label)\n",
        "            cnt += 1\n",
        "#             if cnt % 100 == 0:\n",
        "#                 print('Processed %d sentences' % cnt)\n",
        "    return result_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMjBWpYNaH0n"
      },
      "outputs": [],
      "source": [
        "# pred_file = 't5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "# actual_file = 'test_raw.txt'\n",
        "# pred_file = 'data/ncbi/t5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "# actual_file = 'data/ncbi/test_raw.txt'\n",
        "checkpoint = 237400\n",
        "for task in tasks:\n",
        "    d = task[0]\n",
        "    t = task[1]\n",
        "    \n",
        "    pred_file = os.path.join(t + '_predict_output.txt-%d'%checkpoint)\n",
        "    actual_file = os.path.join(t + '_actual_output.txt')\n",
        "    \n",
        "    # pred_file = 't5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "    # actual_file = 'test_raw.txt'\n",
        "    pred_labels = convert_BIO_labels(pred_file)\n",
        "    actual_labels = convert_BIO_labels(actual_file)\n",
        "    for i, (a, b) in enumerate(zip(pred_labels, actual_labels)):\n",
        "        len_a = len(a)\n",
        "        len_b = len(b)\n",
        "        \n",
        "        if len_a > len_b:\n",
        "            pred_labels[i] = pred_labels[i][:len_b]\n",
        "        elif len_a < len_b:\n",
        "            pred_labels[i] = pred_labels[i] + ['PAD'] * (len_b - len_a)\n",
        "\n",
        "        \n",
        "    f1score = f1_score(actual_labels, pred_labels)\n",
        "    recallscore = recall_score(actual_labels, pred_labels)\n",
        "    precisionscore = precision_score(actual_labels, pred_labels)\n",
        "    \n",
        "#     f1score = f1_score(tmp_actual, tmp_pred)\n",
        "#     recallscore = recall_score(tmp_actual, tmp_pred)\n",
        "#     precisionscore = precision_score(tmp_actual, tmp_pred)\n",
        "    \n",
        "#     print(\"%s\\t Precision: %2f \\t Recall-score: %2f \\t F1-score: %2f \" % (t, precisionscore, recallscore, f1score))\n",
        "#     print(\"Accuracy score: %2f\" % accuracy_score(tmp_actual, tmp_pred))\n",
        "    print(t, 'f1-score', f1score)\n",
        "    print('recallscore', recallscore)\n",
        "    print('precisionscore', precisionscore)\n",
        "    # print(\"Report:\", classification_report(actual_labels, pred_labels, digits=4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "bio_t5_fine_tune_ner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('t0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "462d7afb82e73546fb32fc68c91be82fe4847e6fdf969ffd1e2ed36758d69187"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
