{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "# stategy of inference \n",
    "1. template 单独计算 metric--> comparing the the difference between them\n",
    "2. all template together to caculate the metric\n",
    "3. ensemble: vote OR mean() TO get metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import json\n",
    "def test_data_prepare(path:str, type_1=\"imcs_dac_v2\")->Tuple[list,list]:\n",
    "    \"generate the test dataset from path in ncbi_blue dataset\"\n",
    "    try:\n",
    "        with open(path,\"r\") as f:\n",
    "            content = json.load(f)\n",
    "    except:\n",
    "        raise Exception(\"the file need to be #json# file type!\")\n",
    "    # 每个template 单独 计算 inference metric\n",
    "    print(\"{} test dataset length is {}\".format(path, len(content)))\n",
    "    input_list = []\n",
    "    label_list = []\n",
    "    for i in content:   \n",
    "        tmp = i    \n",
    "        if len(tmp)<=1:\n",
    "            raise Exception(\"input sentence spliting exception!\")\n",
    "        # require len(tmp)>=2 \n",
    "        input = tmp[0]\n",
    "        if type_1==\"ner\":\n",
    "            label = tmp[-1].split(\",\")\n",
    "        elif type_1==\"sentence_pairs\":\n",
    "            label = float(tmp[-1])\n",
    "        elif type_1 == \"imcs_dac_v2\":\n",
    "            label = tmp[-1]\n",
    "        input_list.append(input)\n",
    "        # ,\n",
    "        label_list.append(label)\n",
    "    return input_list, label_list         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = test_data_prepare(\"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/test_cblue_v2/imcs_dac_v2/Classify intent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(inputs_data,batch_size=export_model_batch_size):\n",
    "    \"generator for batch test data\"\n",
    "    for start_idx in range(0, len(inputs_data), batch_size):\n",
    "        excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs_data[excerpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope = list(batch_data(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ope[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "class SelfEncoder(json.JSONEncoder):  \n",
    "    def default(self, obj):  \n",
    "        if isinstance(obj, np.ndarray):  \n",
    "            return obj.tolist()  \n",
    "        elif isinstance(obj, np.floating):  \n",
    "            return float(obj)  \n",
    "        elif isinstance(obj, bytes):  \n",
    "            return str(obj, encoding='utf-8');  \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "def ner_one_template(input_batch:list, model_name=\"dac_v2_3b\"):\n",
    "    '''input:batch_list []\n",
    "        output:predicitons\n",
    "    '''\n",
    "    input = np.array(input_batch)\n",
    "    input_data = {  \n",
    "    \"signature_name\": \"\",  \n",
    "    \"instances\":input}\n",
    "    data = json.dumps(input_data, cls=SelfEncoder, indent=2)\n",
    "    root_url = \"http://10.100.45.205:8501\"\n",
    "    url = f\"{root_url}/v1/models/{model_name}:predict\"\n",
    "    result = requests.post(url, data=data)\n",
    "    tmp = eval(result.content)\n",
    "    return_list = [i[\"outputs\"] for i in tmp[\"predictions\"]]\n",
    "    return return_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "result = []\n",
    "ti = time.time()\n",
    "for i in tqdm(ope[:50]):\n",
    "    result.append(ner_one_template(i))\n",
    "end = time.time()\n",
    "print(end - ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "222.85/(len(result)*len(result[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time  \n",
    "T_pool = Pool(10)\n",
    "ti = time.time()\n",
    "result = T_pool.map(ner_one_template, ope[:400])\n",
    "end = time.time()\n",
    "print(end - ti)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def ner_dataset_test(path:str):\n",
    "    '''param: already prepared dataset_dir-->path\n",
    "       func: collect and return all templates prediction result and label\n",
    "    '''\n",
    "    import os\n",
    "    template_file_pth_list = []\n",
    "    for _,_, files, in os.walk(path):\n",
    "        for template_name in files:\n",
    "            if template_name.endswith(\"json\"):\n",
    "                template_file_pth_list.append(template_name)\n",
    "    all_templates_prediction= {}\n",
    "    dataset_label = []\n",
    "\n",
    "    for i in template_file_pth_list:\n",
    "        tmp_prediction = []\n",
    "        template_path = os.path.join(path, i)\n",
    "        print(template_path)\n",
    "        input, label = test_data_prepare(template_path)\n",
    "        # get label\n",
    "        if dataset_label==[]:\n",
    "            dataset_label=label\n",
    "    \n",
    "        for input_batch in batch_data(input):\n",
    "            tmp_prediction.extend(ner_one_template(input_batch))\n",
    "        # collect all the templates applied test dataset's prediction of this ner dataset\n",
    "        all_templates_prediction[i] = tmp_prediction\n",
    "        print(\"template {} has been finished\".format(i))\n",
    "    # examine the prediction\n",
    "    assert len(template_file_pth_list) == len(all_templates_prediction.keys())\n",
    "    # 返回一个label\n",
    "    return all_templates_prediction, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_one_prompt(prediction, label):\n",
    "    # peasrso 系数, 判断是否线性相关??\n",
    "    # pair task for biosses\n",
    "    right_n = 0\n",
    "    prediction_len = 0\n",
    "    label_len = 0\n",
    "    assert len(label)==len(prediction)\n",
    "    prediction = [i for i in prediction]\n",
    "    label = [j for j in label]\n",
    "    for i,j in zip(prediction, label):\n",
    "        if i.strip()==j.strip():\n",
    "            right_n+=1\n",
    "    acc = float(right_n)/len(label)        \n",
    "    return {\"acc\":acc, \"length_prediction\":len(prediction)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(predictions, label):\n",
    "    all_prediction =[]\n",
    "    for idx in range(len(label)):\n",
    "        tmp = []\n",
    "        for prompt in predictions.keys():\n",
    "            tmp.extend(predictions[prompt][idx])\n",
    "        all_prediction.append(list(set(tmp)))\n",
    "    return all_prediction\n",
    "\n",
    "def vote(predictions, label, threhold=2):\n",
    "    from collections import Counter\n",
    "    result_prediction = []\n",
    "    for idx in range(len(label)):\n",
    "        tmp = []\n",
    "        for prompt in predictions.keys():\n",
    "            tmp.extend(predictions[prompt][idx])\n",
    "        tmp_c = Counter(tmp)\n",
    "        if idx==0:\n",
    "            print(tmp_c)\n",
    "        ## vote strategy >half prompts num\n",
    "        f_tmp = [i[0] for i in tmp_c.items() if i[1]>=threhold]\n",
    "        if idx==0:\n",
    "            print(f_tmp)\n",
    "        result_prediction.append(f_tmp)\n",
    "    return result_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREHOLD = 2 # the threshold num of votes of templates \n",
    "def predict_result(predictions, label):\n",
    "    predict_result = {}\n",
    "    for i in predictions.keys():\n",
    "        predict_result[i] = metric_one_prompt(predictions[i], label) \n",
    "    all_predict = merge(predictions, label) \n",
    "    vote_predict = vote(predictions, label, threhold=2)\n",
    "    predict_result[\"add_all_templates\"] = metric_one_prompt(all_predict, label)\n",
    "    predict_result[\"vote_by_all_templates\"] = metric_one_prompt(vote_predict, label)\n",
    "    return predict_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/test_cblue_v2/{}\"\n",
    "test_dataset_path = {}\n",
    "with open(\"ner_dataset_names\", \"r\") as f:\n",
    "    for i in f:\n",
    "        test_dataset_path[i.strip()]=test_dir.format(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "model_name = \"mt5_3b_imcs_dac_v2\"\n",
    "result_dir = \"/raid/zyftest/project/Med0/t5_multitasks_finetune/eval_metric/test_result\"\n",
    "for dataset_name,path in test_dataset_path.items():\n",
    "    predictions,label_result = ner_dataset_test(path)\n",
    "    tmp = predict_result(predictions, label_result)\n",
    "    save_pth = os.path.join(result_dir, dataset_name+\"_\"+model_name)\n",
    "    print(tmp)\n",
    "    with open(save_pth, \"w\") as f:\n",
    "        json.dump(tmp, f, indent=2, ensure_ascii=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('t0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "462d7afb82e73546fb32fc68c91be82fe4847e6fdf969ffd1e2ed36758d69187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
