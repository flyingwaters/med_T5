{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "# stategy of inference \n",
    "1. template 单独计算 metric--> comparing the the difference between them\n",
    "2. all template together to caculate the metric\n",
    "3. ensemble: vote OR mean() TO get metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import json\n",
    "def test_data_prepare(path:str, type=\"ner\")->Tuple[list,list]:\n",
    "    \"generate the test dataset from path in ncbi_blue dataset\"\n",
    "    try:\n",
    "        with open(path,\"r\") as f:\n",
    "            content = json.load(f)\n",
    "    except:\n",
    "        raise Exception(f\"the file: {path} need to be #json# file type!\")\n",
    "    # 每个template 单独 计算 inference metric\n",
    "    print(\"{} test dataset length is {}\".format(path, len(content)))\n",
    "    input_list = []\n",
    "    label_list = []\n",
    "    for i in content:   \n",
    "        tmp = i.split(\"\\t\")    \n",
    "        if len(tmp)<=1:\n",
    "            raise Exception(\"input sentence spliting exception!\")\n",
    "        # require len(tmp)>=2 \n",
    "        input = \" \".join(tmp[:-1])\n",
    "        if type==\"ner\":\n",
    "            # 分割符号错误\n",
    "            label = tmp[-1].split(\",\")\n",
    "        elif type==\"sentence_pairs\":\n",
    "            label = float(tmp[-1])\n",
    "        input_list.append(input)\n",
    "        # ,\n",
    "        label_list.append(label)\n",
    "    return input_list, label_list         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(inputs_data,batch_size=export_model_batch_size):\n",
    "    \"generator for batch test data\"\n",
    "    for start_idx in range(0, len(inputs_data), batch_size):\n",
    "        excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs_data[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "class SelfEncoder(json.JSONEncoder):  \n",
    "    def default(self, obj):  \n",
    "        if isinstance(obj, np.ndarray):  \n",
    "            return obj.tolist()  \n",
    "        elif isinstance(obj, np.floating):  \n",
    "            return float(obj)  \n",
    "        elif isinstance(obj, bytes):  \n",
    "            return str(obj, encoding='utf-8');  \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "def ner_one_template(input_batch:list, model_name=\"blurb_t5\"):\n",
    "    '''input:batch_list []\n",
    "        output:predicitons\n",
    "    '''\n",
    "    input = np.array(input_batch)\n",
    "    input_data = {  \n",
    "    \"signature_name\": \"\",  \n",
    "    \"instances\":input}\n",
    "    data = json.dumps(input_data, cls=SelfEncoder, indent=2)\n",
    "    root_url = \"http://10.100.45.205:8501\"\n",
    "    url = f\"{root_url}/v1/models/{model_name}:predict\"\n",
    "    result = requests.post(url, data=data)\n",
    "    tmp = eval(result.content)\n",
    "    return_list = [i[\"outputs\"].split(\",\") for i in tmp[\"predictions\"]]\n",
    "    return return_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_dataset_test(path:str):\n",
    "    '''param: already prepared dataset_dir-->path\n",
    "       func: collect and return all templates prediction result and label\n",
    "    '''\n",
    "    import os\n",
    "    template_file_pth_list = []\n",
    "    for _,_, files, in os.walk(path):\n",
    "        for template_name in files:\n",
    "            if template_name.endswith(\"json\"):\n",
    "                template_file_pth_list.append(template_name)\n",
    "    all_templates_prediction= {}\n",
    "    dataset_label = []\n",
    "\n",
    "    for i in template_file_pth_list:\n",
    "        tmp_prediction = []\n",
    "        template_path = os.path.join(path, i)\n",
    "        input, label = test_data_prepare(template_path)\n",
    "        # get label\n",
    "        if dataset_label==[]:\n",
    "            dataset_label=label\n",
    "    \n",
    "        for input_batch in batch_data(input):\n",
    "            tmp_prediction.extend(ner_one_template(input_batch))\n",
    "        # collect all the templates applied test dataset's prediction of this ner dataset\n",
    "        all_templates_prediction[i] = tmp_prediction\n",
    "        print(\"template {} has been finished\".format(i))\n",
    "    # examine the prediction\n",
    "    assert len(template_file_pth_list) == len(all_templates_prediction.keys())\n",
    "    # 返回一个label\n",
    "    return all_templates_prediction, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_one_prompt(prediction, label):\n",
    "    right_n = 0\n",
    "    prediction_len = 0\n",
    "    label_len = 0\n",
    "    for predict_i,label_i in zip(prediction,label):\n",
    "        for entity in predict_i:\n",
    "            if entity in label_i:\n",
    "                right_n+=1\n",
    "        prediction_len+=len(predict_i)\n",
    "        label_len += len(label_i)\n",
    "\n",
    "    acc = float(right_n) / float(prediction_len)*100\n",
    "    recall = float(right_n) / float(label_len)*100\n",
    "    f1 = 2*(acc*recall)/(acc+recall)\n",
    "    return {\"f1\":f1,\"recall\":recall, \"acc\":acc, \"right_num\":right_n, \"label_entities\":label_len, \"prediction_len\":prediction_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(predictions, label):\n",
    "    all_prediction =[]\n",
    "    for idx in range(len(label)):\n",
    "        tmp = []\n",
    "        for prompt in predictions.keys():\n",
    "            tmp.extend(predictions[prompt][idx])\n",
    "        all_prediction.append(list(set(tmp)))\n",
    "    return all_prediction\n",
    "\n",
    "def vote(predictions, label, threhold=2):\n",
    "    from collections import Counter\n",
    "    result_prediction = []\n",
    "    for idx in range(len(label)):\n",
    "        tmp = []\n",
    "        for prompt in predictions.keys():\n",
    "            tmp.extend(predictions[prompt][idx])\n",
    "        tmp_c = Counter(tmp)\n",
    "        if idx==0:\n",
    "            print(tmp_c)\n",
    "        ## vote strategy >half prompts num\n",
    "        f_tmp = [i[0] for i in tmp_c.items() if i[1]>=threhold]\n",
    "        if idx==0:\n",
    "            print(f_tmp)\n",
    "        result_prediction.append(list(f_tmp))\n",
    "    return result_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREHOLD = 2 # the threshold num of votes of templates \n",
    "def predict_result(predictions, label):\n",
    "    predict_result = {}\n",
    "    for i in predictions.keys():\n",
    "        predict_result[i] = metric_one_prompt(predictions[i], label) \n",
    "    all_predict = merge(predictions, label) \n",
    "    vote_predict = vote(predictions, label, threhold=2)\n",
    "    predict_result[\"add_all_templates\"] = metric_one_prompt(all_predict, label)\n",
    "    predict_result[\"vote_by_all_templates\"] = metric_one_prompt(vote_predict, label)\n",
    "    return predict_result,all_predict,vote_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/raid/yiptmp/nlp_prepare_dataset/med0_dataset/blurb_test_dataset/{}\"\n",
    "test_dataset_path = {}\n",
    "with open(\"ner_dataset_names\", \"r\") as f:\n",
    "    for i in f:\n",
    "        test_dataset_path[i.strip()]=test_dir.format(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体级别的测试指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "model_name = \"t5_base\"\n",
    "result_dir = \"/raid/zyftest/project/Med0/t5_multitasks_finetune/eval_metric/test_result\"\n",
    "for dataset_name,path in test_dataset_path.items():\n",
    "    predictions,label = ner_dataset_test(path)\n",
    "    tmp,all_tem_prediction,vote_tem_prediction = predict_result(predictions, label)\n",
    "    # save the prediction\n",
    "    predictions[\"all_templates_output\"] = all_tem_prediction\n",
    "    predictions[\"vote_templates_output\"] = vote_tem_prediction\n",
    "    content = {\"predictions\":predictions, \"label\":label}\n",
    "    with open(path+\"/\"+f\"{model_name}_output_result\",\"w\") as result_w:\n",
    "        json.dump(content, result_w, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    \n",
    "    save_dir_path = os.path.join(result_dir, model_name)\n",
    "    save_pth = os.path.join(save_dir_path, dataset_name)\n",
    "    if not os.path.exists(save_dir_path):\n",
    "        os.makedirs(save_dir_path)\n",
    "    \n",
    "    with open(save_pth, \"w\") as f:\n",
    "        json.dump(tmp, f, indent=2, ensure_ascii=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('t0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "462d7afb82e73546fb32fc68c91be82fe4847e6fdf969ffd1e2ed36758d69187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
